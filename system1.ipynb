{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A. System 1 Semantic Coding Model"
      ],
      "metadata": {
        "id": "6p7TO39eGbfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Action-Concept **Relevance**"
      ],
      "metadata": {
        "id": "o6THB7hLBUnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# System 1 SNC: Action–Concept Relevance (Eq. 1)\n",
        "# p(X|A=a; t) = Π_c p(X_c | A=a; t),  X_c ∈ {True, False}\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class ActionConceptRelevanceModel:\n",
        "    \"\"\"\n",
        "    Implements Eq. (1) in the paper:\n",
        "        p(X|A=a; t) = Π_{c∈C} p(X_c | A=a; t)\n",
        "    where X_c is Bernoulli (True/False).\n",
        "\n",
        "    Internally we store:\n",
        "        p_true[t, a, c] = P(X_c = True | A=a; t)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tasks: int, num_actions: int, num_concepts: int, seed: int = 0):\n",
        "        self.T = num_tasks\n",
        "        self.A = num_actions\n",
        "        self.C = num_concepts\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        # p_true[t, a, c] ∈ (0,1)\n",
        "        # You can replace this initialization with learned logits from a neural model.\n",
        "        self.p_true = None\n",
        "\n",
        "    def random_init_beta(self, alpha: float = 0.7, beta: float = 0.7):\n",
        "        \"\"\"\n",
        "        Randomly initialize P(X_c=True|a;t) using a Beta distribution.\n",
        "        Smaller alpha/beta -> more extreme probabilities near 0 or 1.\n",
        "        \"\"\"\n",
        "        self.p_true = self.rng.beta(alpha, beta, size=(self.T, self.A, self.C)).astype(np.float32)\n",
        "        self._clip_inplace()\n",
        "        return self.p_true\n",
        "\n",
        "    def set_p_true(self, p_true: np.ndarray):\n",
        "        \"\"\"\n",
        "        Set p_true explicitly. Expected shape: (T, A, C)\n",
        "        \"\"\"\n",
        "        assert p_true.shape == (self.T, self.A, self.C)\n",
        "        self.p_true = p_true.astype(np.float32)\n",
        "        self._clip_inplace()\n",
        "\n",
        "    def _clip_inplace(self, eps: float = 1e-8):\n",
        "        # avoid log(0)\n",
        "        self.p_true = np.clip(self.p_true, eps, 1.0 - eps)\n",
        "\n",
        "    def sample_X(self, a: int, t: int, n: int = 1) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample X = (X_1,...,X_C) for a fixed action a and task t.\n",
        "        Returns boolean array of shape (n, C).\n",
        "        \"\"\"\n",
        "        assert self.p_true is not None, \"Call random_init_beta() or set_p_true() first.\"\n",
        "        p = self.p_true[t, a]  # (C,)\n",
        "        # Bernoulli sampling\n",
        "        u = self.rng.random(size=(n, self.C))\n",
        "        X = (u < p)\n",
        "        return X\n",
        "\n",
        "    def log_prob_X(self, X: np.ndarray, a: int, t: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute log p(X | A=a; t) under conditional independence.\n",
        "        X can be shape (C,) or (n, C). Returns shape () or (n,).\n",
        "        \"\"\"\n",
        "        assert self.p_true is not None, \"Call random_init_beta() or set_p_true() first.\"\n",
        "        p = self.p_true[t, a]  # (C,)\n",
        "        X = np.asarray(X)\n",
        "        if X.ndim == 1:\n",
        "            X = X[None, :]  # (1, C)\n",
        "\n",
        "        # log p = Σ_c [ x_c log p_c + (1-x_c) log (1-p_c) ]\n",
        "        x = X.astype(np.float32)\n",
        "        logp = (x * np.log(p) + (1.0 - x) * np.log(1.0 - p)).sum(axis=1)\n",
        "        return logp if logp.shape[0] > 1 else logp[0]\n",
        "\n",
        "    def prob_X(self, X: np.ndarray, a: int, t: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute p(X | A=a; t) (may underflow for large C).\n",
        "        Prefer log_prob_X for stability.\n",
        "        \"\"\"\n",
        "        return np.exp(self.log_prob_X(X, a, t))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Demo / sanity check\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    num_tasks = 3\n",
        "    num_actions = 5\n",
        "    num_concepts = 8\n",
        "\n",
        "    model = ActionConceptRelevanceModel(num_tasks, num_actions, num_concepts, seed=42)\n",
        "    model.random_init_beta(alpha=0.6, beta=0.6)\n",
        "\n",
        "    t = 1\n",
        "    a = 2\n",
        "    X_samples = model.sample_X(a=a, t=t, n=4)\n",
        "    print(\"Sampled X (rows are samples, cols are concepts):\\n\", X_samples.astype(int))\n",
        "    print(\"\\nlog p(X|a,t) for each sample:\\n\", model.log_prob_X(X_samples, a=a, t=t))\n",
        "\n",
        "    # Factorization sanity: show that joint log-prob equals sum of per-concept terms\n",
        "    X0 = X_samples[0].astype(np.float32)\n",
        "    p = model.p_true[t, a]\n",
        "    per_c = X0 * np.log(p) + (1 - X0) * np.log(1 - p)\n",
        "    print(\"\\nCheck factorization (difference should be ~0):\",\n",
        "          float(model.log_prob_X(X0, a=a, t=t) - per_c.sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSzkRONZAA6O",
        "outputId": "f7e8475f-855c-4c12-a0c2-49652aca1573"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled X (rows are samples, cols are concepts):\n",
            " [[1 0 0 0 0 1 1 0]\n",
            " [1 0 0 0 0 1 0 1]\n",
            " [1 0 0 1 0 1 1 0]\n",
            " [1 0 0 0 0 1 1 1]]\n",
            "\n",
            "log p(X|a,t) for each sample:\n",
            " [-1.1806356 -2.80388   -2.6009328 -2.1326973]\n",
            "\n",
            "Check factorization (difference should be ~0): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Action-Concept Model"
      ],
      "metadata": {
        "id": "UZqTAQSYBguW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# System 1 SNC: (1) Action–Concept Relevance  ->  (2) A2C  ->  (3) C2A\n",
        "# Colab-ready, numpy-only implementation\n",
        "# =========================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def logsumexp(x, axis=None, keepdims=False):\n",
        "    x = np.asarray(x)\n",
        "    m = np.max(x, axis=axis, keepdims=True)\n",
        "    y = m + np.log(np.sum(np.exp(x - m), axis=axis, keepdims=True))\n",
        "    return y if keepdims else np.squeeze(y, axis=axis)\n",
        "\n",
        "class System1_SNC:\n",
        "    \"\"\"\n",
        "    Implements:\n",
        "      (1) p(X|A=a;t) = Π_c p(X_c | A=a;t), with Bernoulli X_c\n",
        "          store p_true[t,a,c] = P(X_c=TRUE | a,t)\n",
        "\n",
        "      (2) A2C:  p(C=c | A=a; t) = p_xc|A(TRUE | a,t) / Σ_{c'} p_xc'|A(TRUE | a,t)\n",
        "\n",
        "      (3) C2A via Bayes:\n",
        "          p(A=a | C=c; t) = p(C=c | A=a;t) p_A(a) / Σ_{a'} p(C=c | A=a';t) p_A(a')\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tasks: int, num_actions: int, num_concepts: int, seed: int = 0):\n",
        "        self.T = num_tasks\n",
        "        self.A = num_actions\n",
        "        self.C = num_concepts\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        # p_true[t,a,c] = P(X_c=TRUE | a,t)\n",
        "        self.p_true = None\n",
        "\n",
        "    # ---------- (1) Action–Concept Relevance ----------\n",
        "    def random_init_beta(self, alpha: float = 0.7, beta: float = 0.7, eps: float = 1e-8):\n",
        "        self.p_true = self.rng.beta(alpha, beta, size=(self.T, self.A, self.C)).astype(np.float32)\n",
        "        self.p_true = np.clip(self.p_true, eps, 1.0 - eps)\n",
        "        return self.p_true\n",
        "\n",
        "    def set_p_true(self, p_true: np.ndarray, eps: float = 1e-8):\n",
        "        assert p_true.shape == (self.T, self.A, self.C)\n",
        "        self.p_true = np.clip(p_true.astype(np.float32), eps, 1.0 - eps)\n",
        "\n",
        "    def sample_X(self, a: int, t: int, n: int = 1) -> np.ndarray:\n",
        "        \"\"\"Sample X vector(s) under (1). Returns bool array (n,C).\"\"\"\n",
        "        assert self.p_true is not None\n",
        "        p = self.p_true[t, a]  # (C,)\n",
        "        u = self.rng.random(size=(n, self.C))\n",
        "        return (u < p)\n",
        "\n",
        "    def log_prob_X(self, X: np.ndarray, a: int, t: int) -> np.ndarray:\n",
        "        \"\"\"Compute log p(X|a,t) under conditional independence (1).\"\"\"\n",
        "        assert self.p_true is not None\n",
        "        p = self.p_true[t, a]  # (C,)\n",
        "        X = np.asarray(X)\n",
        "        if X.ndim == 1:\n",
        "            X = X[None, :]\n",
        "        x = X.astype(np.float32)\n",
        "        logp = (x * np.log(p) + (1.0 - x) * np.log(1.0 - p)).sum(axis=1)\n",
        "        return logp if logp.shape[0] > 1 else logp[0]\n",
        "\n",
        "    # ---------- (2) Action-to-Concept (A2C) ----------\n",
        "    def a2c_probs(self, a: int, t: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Eq (2): p(C=c | A=a; t) by normalizing p_true[t,a,c] over concepts.\n",
        "        Returns (C,) probabilities summing to 1.\n",
        "        \"\"\"\n",
        "        assert self.p_true is not None\n",
        "        w = self.p_true[t, a].astype(np.float64)  # (C,)\n",
        "        s = np.sum(w)\n",
        "        # If s is (numerically) zero, fall back to uniform (shouldn't happen due to clipping)\n",
        "        if s <= 0:\n",
        "            return np.ones(self.C, dtype=np.float64) / self.C\n",
        "        return (w / s).astype(np.float64)\n",
        "\n",
        "    def a2c_log_probs(self, a: int, t: int) -> np.ndarray:\n",
        "        \"\"\"Log-space version of Eq (2) for stability.\"\"\"\n",
        "        assert self.p_true is not None\n",
        "        logw = np.log(self.p_true[t, a].astype(np.float64))  # (C,)\n",
        "        return logw - logsumexp(logw, axis=0)\n",
        "\n",
        "    def sample_C(self, a: int, t: int, n: int = 1) -> np.ndarray:\n",
        "        \"\"\"Sample concept index C ~ p(C|A=a;t) from Eq (2). Returns (n,) int.\"\"\"\n",
        "        pC = self.a2c_probs(a, t)\n",
        "        return self.rng.choice(self.C, size=n, p=pC)\n",
        "\n",
        "    # ---------- (3) Concept-to-Action (C2A) via Bayes ----------\n",
        "    def c2a_posterior(self, c: int, t: int, pA: np.ndarray = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Eq (3): p(A=a | C=c; t) ∝ p(C=c | A=a; t) * p_A(a)\n",
        "        Returns (A,) posterior over actions.\n",
        "        \"\"\"\n",
        "        assert self.p_true is not None\n",
        "        if pA is None:\n",
        "            pA = np.ones(self.A, dtype=np.float64) / self.A\n",
        "        else:\n",
        "            pA = np.asarray(pA, dtype=np.float64)\n",
        "            assert pA.shape == (self.A,)\n",
        "            # normalize prior just in case\n",
        "            pA = pA / np.sum(pA)\n",
        "\n",
        "        # compute p(C=c | A=a;t) for all actions a\n",
        "        # using Eq (2): normalize p_true[t,a,:] across concepts\n",
        "        w = self.p_true[t].astype(np.float64)          # shape (A, C)\n",
        "        denom = np.sum(w, axis=1, keepdims=True)       # shape (A, 1)\n",
        "        pC_given_A = (w / denom)[:, c]                 # shape (A,)\n",
        "\n",
        "        unnorm = pC_given_A * pA                       # shape (A,)\n",
        "        Z = np.sum(unnorm)\n",
        "        if Z <= 0:\n",
        "            return np.ones(self.A, dtype=np.float64) / self.A\n",
        "        return unnorm / Z\n",
        "\n",
        "    def c2a_log_posterior(self, c: int, t: int, pA: np.ndarray = None) -> np.ndarray:\n",
        "        \"\"\"Log-space Bayes posterior (recommended if A is large).\"\"\"\n",
        "        assert self.p_true is not None\n",
        "        if pA is None:\n",
        "            logpA = -np.log(self.A) * np.ones(self.A, dtype=np.float64)\n",
        "        else:\n",
        "            pA = np.asarray(pA, dtype=np.float64)\n",
        "            pA = pA / np.sum(pA)\n",
        "            logpA = np.log(np.clip(pA, 1e-300, 1.0))\n",
        "\n",
        "        # log p(C=c | A=a;t)\n",
        "        log_pC_given_A = np.zeros(self.A, dtype=np.float64)\n",
        "        for a in range(self.A):\n",
        "            log_pC_given_A[a] = self.a2c_log_probs(a, t)[c]\n",
        "\n",
        "        log_post_unnorm = log_pC_given_A + logpA\n",
        "        return log_post_unnorm - logsumexp(log_post_unnorm, axis=0)\n",
        "\n",
        "    def infer_action_map(self, c: int, t: int, pA: np.ndarray = None) -> int:\n",
        "        \"\"\"MAP action: argmax_a p(A=a | C=c; t).\"\"\"\n",
        "        post = self.c2a_posterior(c, t, pA=pA)\n",
        "        return int(np.argmax(post))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Quick demo (runs in Colab)\n",
        "# =========================================================\n",
        "if __name__ == \"__main__\":\n",
        "    T, A, C = 2, 5, 8\n",
        "    sys1 = System1_SNC(num_tasks=T, num_actions=A, num_concepts=C, seed=123)\n",
        "    sys1.random_init_beta(alpha=0.6, beta=0.6)\n",
        "\n",
        "    t = 0\n",
        "    a_true = 3\n",
        "\n",
        "    # Speaker: pick concept using A2C (Eq 2)\n",
        "    c_sent = sys1.sample_C(a_true, t, n=1)[0]\n",
        "    print(f\"[Speaker] task t={t}, intended action a={a_true} -> sent concept c={c_sent}\")\n",
        "\n",
        "    # Listener: infer action using C2A posterior (Eq 3)\n",
        "    # Example prior: slightly favors action 0\n",
        "    pA = np.array([0.40, 0.15, 0.15, 0.15, 0.15], dtype=np.float64)\n",
        "    post = sys1.c2a_posterior(c_sent, t, pA=pA)\n",
        "    a_hat = int(np.argmax(post))\n",
        "    print(f\"[Listener] posterior p(A|c,t) = {np.round(post, 4)}\")\n",
        "    print(f\"[Listener] MAP action = {a_hat}\")\n",
        "\n",
        "    # (Optional) also sample an X vector under Eq (1) for the same a,t\n",
        "    X = sys1.sample_X(a_true, t, n=1)[0]\n",
        "    print(f\"[Eq(1) sample] X (as 0/1 over concepts): {X.astype(int)}\")\n",
        "    print(f\"[Eq(1) log p(X|a,t)] {float(sys1.log_prob_X(X, a_true, t)):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl46gizLBjM2",
        "outputId": "2a491500-4cb5-4b6d-e60d-b56ecfb5f44d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Speaker] task t=0, intended action a=3 -> sent concept c=1\n",
            "[Listener] posterior p(A|c,t) = [0.267  0.3176 0.0884 0.2843 0.0428]\n",
            "[Listener] MAP action = 1\n",
            "[Eq(1) sample] X (as 0/1 over concepts): [1 1 0 0 0 0 1 0]\n",
            "[Eq(1) log p(X|a,t)] -2.5596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Concept-Symbol Model"
      ],
      "metadata": {
        "id": "RxUGGuhGDixn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "# =========================================================\n",
        "# System 1 core: (1)(2)(3) 중 (1)과 (2)까지만 SR 만들 때 사용\n",
        "# (3) C2A는 listener 쪽이라 Shannon 이후에 붙이면 됨.\n",
        "# =========================================================\n",
        "class System1_SNC:\n",
        "    \"\"\"\n",
        "    Stores p_true[t,a,c] = P(X_c=TRUE | A=a; t)\n",
        "\n",
        "    (2) A2C:\n",
        "      p(C=c | A=a; t) = p_true[t,a,c] / sum_{c'} p_true[t,a,c']\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tasks: int, num_actions: int, num_concepts: int, seed: int = 0):\n",
        "        self.T = num_tasks\n",
        "        self.A = num_actions\n",
        "        self.C = num_concepts\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.p_true = None  # (T, A, C)\n",
        "\n",
        "    def random_init_beta(self, alpha: float = 0.7, beta: float = 0.7, eps: float = 1e-8):\n",
        "        self.p_true = self.rng.beta(alpha, beta, size=(self.T, self.A, self.C)).astype(np.float64)\n",
        "        self.p_true = np.clip(self.p_true, eps, 1.0 - eps)\n",
        "        return self.p_true\n",
        "\n",
        "    def a2c_probs(self, a: int, t: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Eq (2): normalize p_true[t,a,:] across concepts to get p(C|A=a;t).\n",
        "        \"\"\"\n",
        "        assert self.p_true is not None, \"Initialize p_true first.\"\n",
        "        w = self.p_true[t, a]  # (C,)\n",
        "        return w / np.sum(w)\n",
        "\n",
        "    def sample_concepts(self, a: int, t: int, k: int = 1, replace: bool = False) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample k concepts from A2C distribution. (Paper uses single concept; k=1 is default.)\n",
        "        \"\"\"\n",
        "        p = self.a2c_probs(a, t)\n",
        "        idx = self.rng.choice(self.C, size=k, replace=replace, p=p)\n",
        "        return idx\n",
        "\n",
        "    def topk_concepts(self, a: int, t: int, k: int = 1) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Deterministic alternative: pick top-k concepts by A2C probability.\n",
        "        \"\"\"\n",
        "        p = self.a2c_probs(a, t)\n",
        "        return np.argsort(-p)[:k]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# (3) Concept-Symbol Model: 중앙 코드북, deterministic one-to-one\n",
        "# =========================================================\n",
        "@dataclass(frozen=True)\n",
        "class CentralCodebookC2S:\n",
        "    \"\"\"\n",
        "    Deterministic one-to-one mapping s: C -> S\n",
        "    We implement it as two dicts: c2s and s2c.\n",
        "    \"\"\"\n",
        "    c2s: Dict[int, Any]   # concept_id -> symbol (token)\n",
        "    s2c: Dict[Any, int]   # symbol -> concept_id\n",
        "\n",
        "    @staticmethod\n",
        "    def make_identity(num_concepts: int):\n",
        "        # symbol을 정수로 쓰는 가장 간단한 코드북: s(c)=c\n",
        "        c2s = {c: c for c in range(num_concepts)}\n",
        "        s2c = {c: c for c in range(num_concepts)}\n",
        "        return CentralCodebookC2S(c2s=c2s, s2c=s2c)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_tokenized(num_concepts: int, prefix: str = \"S\"):\n",
        "        # symbol을 문자열 토큰으로: s(c) = \"S_0007\" 같은 형태\n",
        "        c2s = {c: f\"{prefix}_{c:04d}\" for c in range(num_concepts)}\n",
        "        s2c = {v: k for k, v in c2s.items()}\n",
        "        return CentralCodebookC2S(c2s=c2s, s2c=s2c)\n",
        "\n",
        "    def encode_concepts_to_symbols(self, concepts: np.ndarray) -> List[Any]:\n",
        "        return [self.c2s[int(c)] for c in concepts]\n",
        "\n",
        "    def decode_symbols_to_concepts(self, symbols: List[Any]) -> np.ndarray:\n",
        "        return np.array([self.s2c[s] for s in symbols], dtype=int)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Semantic Encoder (System 1 기준): action -> concept(s) -> symbol(s) = SR\n",
        "# Shannon 코딩 단계 \"직전\" 출력 포맷까지\n",
        "# =========================================================\n",
        "@dataclass\n",
        "class SemanticRepresentation:\n",
        "    \"\"\"\n",
        "    SR: the semantic representation of action a, i.e., a set/list of symbols\n",
        "    obtained via A2C and C2S.\n",
        "    \"\"\"\n",
        "    task_t: int\n",
        "    action_a: int\n",
        "    concepts: List[int]\n",
        "    symbols: List[Any]\n",
        "    a2c_probs: Optional[np.ndarray] = None  # (C,) optional for analysis/debug\n",
        "\n",
        "\n",
        "class System1SemanticEncoder:\n",
        "    def __init__(self, sys1: System1_SNC, codebook: CentralCodebookC2S):\n",
        "        self.sys1 = sys1\n",
        "        self.codebook = codebook\n",
        "\n",
        "    def encode(self, a: int, t: int, k: int = 1, mode: str = \"sample\") -> SemanticRepresentation:\n",
        "        \"\"\"\n",
        "        mode:\n",
        "          - \"sample\": sample k concepts from A2C\n",
        "          - \"topk\"  : pick top-k concepts deterministically\n",
        "        output:\n",
        "          SR.symbols -> 이걸 다음 단계 Shannon(bit encoder)의 입력으로 넘기면 됨\n",
        "        \"\"\"\n",
        "        if mode == \"sample\":\n",
        "            concepts = self.sys1.sample_concepts(a=a, t=t, k=k, replace=False)\n",
        "        elif mode == \"topk\":\n",
        "            concepts = self.sys1.topk_concepts(a=a, t=t, k=k)\n",
        "        else:\n",
        "            raise ValueError(\"mode must be one of ['sample', 'topk'].\")\n",
        "\n",
        "        symbols = self.codebook.encode_concepts_to_symbols(concepts)\n",
        "        p = self.sys1.a2c_probs(a, t)\n",
        "\n",
        "        return SemanticRepresentation(\n",
        "            task_t=t,\n",
        "            action_a=a,\n",
        "            concepts=[int(x) for x in concepts],\n",
        "            symbols=symbols,\n",
        "            a2c_probs=p\n",
        "        )\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Demo: Shannon 코딩 '직전'까지 확인\n",
        "# =========================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # toy sizes\n",
        "    T, A, C = 2, 5, 12\n",
        "\n",
        "    # 1) System 1 relevance model 준비 (p_true)\n",
        "    sys1 = System1_SNC(num_tasks=T, num_actions=A, num_concepts=C, seed=123)\n",
        "    sys1.random_init_beta(alpha=0.6, beta=0.6)\n",
        "\n",
        "    # 2) 중앙 코드북(C2S): 1:1 deterministic\n",
        "    #   - 정수 심볼로 할지, 문자열 토큰으로 할지 선택\n",
        "    codebook = CentralCodebookC2S.make_tokenized(num_concepts=C, prefix=\"SYM\")\n",
        "\n",
        "    # 3) Semantic encoder: action -> SR(symbols)  (Shannon 직전)\n",
        "    sem_enc = System1SemanticEncoder(sys1=sys1, codebook=codebook)\n",
        "\n",
        "    t = 0\n",
        "    a = 3\n",
        "\n",
        "    # paper는 single concept이 기본이지만, k>1로 \"symbol set\"도 만들 수 있게 해둠\n",
        "    sr = sem_enc.encode(a=a, t=t, k=3, mode=\"sample\")\n",
        "\n",
        "    print(\"=== Semantic Encoding (System 1) ===\")\n",
        "    print(f\"task t={sr.task_t}, action a={sr.action_a}\")\n",
        "    print(f\"chosen concepts: {sr.concepts}\")\n",
        "    print(f\"SR symbols (Shannon input): {sr.symbols}\")\n",
        "\n",
        "    # Shannon coding 단계는 여기서부터!\n",
        "    print(\"\\n--- Next step placeholder (NOT implemented here) ---\")\n",
        "    print(\"bitstream = shannon_bit_encoder.encode(sr.symbols)\")\n",
        "    print(\"rx_bits   = noisy_channel(bitstream)\")\n",
        "    print(\"rx_symbols= shannon_bit_decoder.decode(rx_bits)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF4vogVFGPJ9",
        "outputId": "a67228d7-d65c-4837-9708-8322bd39fe2d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Semantic Encoding (System 1) ===\n",
            "task t=0, action a=3\n",
            "chosen concepts: [3, 11, 5]\n",
            "SR symbols (Shannon input): ['SYM_0003', 'SYM_0011', 'SYM_0005']\n",
            "\n",
            "--- Next step placeholder (NOT implemented here) ---\n",
            "bitstream = shannon_bit_encoder.encode(sr.symbols)\n",
            "rx_bits   = noisy_channel(bitstream)\n",
            "rx_symbols= shannon_bit_decoder.decode(rx_bits)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Shannon Coding under System 1 Semantic Coding"
      ],
      "metadata": {
        "id": "3TObp_C0IWKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "# =========================================================\n",
        "# Shannon coding under System 1 semantic coding\n",
        "# Theorem 1: Expected SR bit-length bounds (Eq. 4,5)\n",
        "#\n",
        "# p_Xc(TRUE) = Σ_a p_Xc|A(TRUE|a; t) p_A(a)\n",
        "# q_c = p_Xc(TRUE) / Σ_{c'} p_Xc'(TRUE)\n",
        "#\n",
        "# Lower bound:\n",
        "#   L_S1 ≥ - Σ_c p_Xc(TRUE) log2( q_c )\n",
        "#\n",
        "# Upper bound:\n",
        "#   L_S1 ≤ - Σ_c p_Xc(TRUE) floor_like? (paper uses [·] as integer length)\n",
        "# We implement the common Shannon code upper bound with ceil:\n",
        "#   l_c = ceil(-log2 q_c)\n",
        "#   L_S1 ≤ Σ_c p_Xc(TRUE) l_c\n",
        "# =========================================================\n",
        "\n",
        "@dataclass\n",
        "class ExpectedSRBitLength:\n",
        "    t: int\n",
        "    pA: np.ndarray          # (A,)\n",
        "    p_x_true: np.ndarray    # (C,)  p_Xc(TRUE)\n",
        "    q: np.ndarray           # (C,)  normalized frequency q_c\n",
        "    denom: float            # Σ_c p_Xc(TRUE)\n",
        "    L_lower: float          # Eq (4)\n",
        "    L_upper: float          # Eq (5) (Shannon length version)\n",
        "    entropy_q: float        # H(q) in bits\n",
        "    avg_shannon_len: float  # Σ_c q_c ceil(-log2 q_c)\n",
        "\n",
        "\n",
        "def expected_sr_bit_length_theorem1(\n",
        "    sys1: System1_SNC,\n",
        "    t: int,\n",
        "    pA: Optional[np.ndarray] = None,\n",
        "    eps: float = 1e-12\n",
        ") -> ExpectedSRBitLength:\n",
        "    \"\"\"\n",
        "    Computes Theorem 1 bounds using sys1.p_true[t,a,c] and action prior pA(a).\n",
        "\n",
        "    This function is designed to plug directly after your SR generation code:\n",
        "      - It uses the same sys1 object\n",
        "      - It does NOT depend on Shannon encoder implementation (bit encoder), only on the distribution.\n",
        "    \"\"\"\n",
        "    assert sys1.p_true is not None, \"sys1.p_true must be initialized first.\"\n",
        "\n",
        "    A = sys1.A\n",
        "    C = sys1.C\n",
        "\n",
        "    if pA is None:\n",
        "        pA = np.ones(A, dtype=np.float64) / A\n",
        "    else:\n",
        "        pA = np.asarray(pA, dtype=np.float64)\n",
        "        assert pA.shape == (A,)\n",
        "        pA = pA / np.sum(pA)\n",
        "\n",
        "    # p_x_true[c] = Σ_a p_true[t,a,c] * pA[a]\n",
        "    p_x_true = (sys1.p_true[t] * pA[:, None]).sum(axis=0)  # shape (C,)\n",
        "\n",
        "    denom = float(np.sum(p_x_true))  # Σ_c p_Xc(TRUE)\n",
        "    if denom <= 0:\n",
        "        # 극단적인 경우(거의 안나옴): 균일 분포로 fallback\n",
        "        q = np.ones(C, dtype=np.float64) / C\n",
        "        p_x_true = q.copy()\n",
        "        denom = 1.0\n",
        "    else:\n",
        "        q = p_x_true / denom\n",
        "\n",
        "    # Numerical safety\n",
        "    q = np.clip(q, eps, 1.0)\n",
        "\n",
        "    # Eq (4): L_lower = - Σ_c p_Xc(TRUE) log2(q_c)\n",
        "    L_lower = float(-np.sum(p_x_true * np.log2(q)))\n",
        "\n",
        "    # Eq (5): use Shannon integer lengths l_c = ceil(-log2 q_c)\n",
        "    l_shannon = np.ceil(-np.log2(q))\n",
        "    L_upper = float(np.sum(p_x_true * l_shannon))\n",
        "\n",
        "    entropy_q = float(-np.sum(q * np.log2(q)))\n",
        "    avg_shannon_len = float(np.sum(q * l_shannon))\n",
        "\n",
        "    return ExpectedSRBitLength(\n",
        "        t=t,\n",
        "        pA=pA,\n",
        "        p_x_true=p_x_true,\n",
        "        q=q,\n",
        "        denom=denom,\n",
        "        L_lower=L_lower,\n",
        "        L_upper=L_upper,\n",
        "        entropy_q=entropy_q,\n",
        "        avg_shannon_len=avg_shannon_len\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# (Optional) SR \"realized\" bit-length proxy for ONE SR sample\n",
        "# - This is NOT Theorem 1; it's a convenient measurement for a realized SR.\n",
        "# - It uses q_c to assign ideal Shannon lengths per symbol and sums them.\n",
        "# =========================================================\n",
        "def realized_sr_shannon_length(sr: SemanticRepresentation, q: np.ndarray, codebook: CentralCodebookC2S) -> float:\n",
        "    \"\"\"\n",
        "    Given one SR (list of symbols), compute sum of ideal Shannon lengths:\n",
        "        length(sr) ≈ Σ_{symbol in SR} ceil(-log2 q_c(symbol))\n",
        "    where c(symbol) is the concept corresponding to that symbol.\n",
        "    \"\"\"\n",
        "    # map symbols -> concepts\n",
        "    concepts = codebook.decode_symbols_to_concepts(sr.symbols)\n",
        "    q_safe = np.clip(q, 1e-12, 1.0)\n",
        "    lengths = np.ceil(-np.log2(q_safe[concepts]))\n",
        "    return float(np.sum(lengths))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Demo that plugs into YOUR previous cell variables: sys1, codebook, sem_enc, sr\n",
        "# =========================================================\n",
        "# 아래는 \"이전 셀\"을 실행한 상태에서 그대로 실행하면 동작하게 만든 예시야.\n",
        "try:\n",
        "    # 예시 prior: uniform or you can set your own pA\n",
        "    pA_demo = np.ones(sys1.A, dtype=np.float64) / sys1.A\n",
        "\n",
        "    res = expected_sr_bit_length_theorem1(sys1=sys1, t=sr.task_t, pA=pA_demo)\n",
        "\n",
        "    print(\"\\n=== Theorem 1: Expected bit-length of SR (System 1 SNC) ===\")\n",
        "    print(f\"task t = {res.t}\")\n",
        "    print(f\"denom = Σ_c p_Xc(TRUE) = {res.denom:.6f}  (scale ~ expected #extracted concepts)\")\n",
        "    print(f\"H(q) = {res.entropy_q:.6f} bits\")\n",
        "    print(f\"Lower bound (Eq.4) L_S1 >= {res.L_lower:.6f} bits\")\n",
        "    print(f\"Upper bound (Eq.5) L_S1 <= {res.L_upper:.6f} bits  (Shannon ceil lengths)\")\n",
        "    print(f\"Avg Shannon len (normalized) = {res.avg_shannon_len:.6f} bits/symbol\")\n",
        "\n",
        "    # (선택) 방금 뽑은 sr 하나에 대해 '근사' 비트길이도 찍어보기\n",
        "    realized_len = realized_sr_shannon_length(sr, res.q, codebook)\n",
        "    print(f\"\\nRealized SR approx. Shannon length (for this SR sample) = {realized_len:.2f} bits\")\n",
        "\n",
        "    print(\"\\n--- Next (bit encoder) connection point ---\")\n",
        "    print(\"bitstream = source_encoder.encode(sr.symbols)  # e.g., Huffman/Arithmetic using q\")\n",
        "    print(\"... then channel coding / noisy channel / decoding ...\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(\"You need to run the previous cell first (sys1, sr, codebook must exist).\")\n",
        "    print(\"Missing name:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7eJzFZ9I8rm",
        "outputId": "8456b497-f5c8-44f3-d715-68e6d7a87aff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Theorem 1: Expected bit-length of SR (System 1 SNC) ===\n",
            "task t = 0\n",
            "denom = Σ_c p_Xc(TRUE) = 5.840479  (scale ~ expected #extracted concepts)\n",
            "H(q) = 3.549122 bits\n",
            "Lower bound (Eq.4) L_S1 >= 20.728574 bits\n",
            "Upper bound (Eq.5) L_S1 <= 23.722610 bits  (Shannon ceil lengths)\n",
            "Avg Shannon len (normalized) = 4.061758 bits/symbol\n",
            "\n",
            "Realized SR approx. Shannon length (for this SR sample) = 12.00 bits\n",
            "\n",
            "--- Next (bit encoder) connection point ---\n",
            "bitstream = source_encoder.encode(sr.symbols)  # e.g., Huffman/Arithmetic using q\n",
            "... then channel coding / noisy channel / decoding ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Shannon communication"
      ],
      "metadata": {
        "id": "qiMK9EjHJ1pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# System 1 SNC: Full end-to-end (simple) simulation with\n",
        "# - Semantic encoder: action -> concept(s) -> symbol(s) (SR)\n",
        "# - Source encoder: Huffman (lossless source coding) on symbols\n",
        "# - Channel encoder: repetition code (simple channel coding)\n",
        "# - Noisy channel: BSC (bit-flip) as a proxy for noise/fading\n",
        "# - Channel decoder: majority vote\n",
        "# - Source decoder: Huffman decode -> recovered symbols\n",
        "# - Semantic decoder: symbols -> concepts -> action (C2A via Bayes)\n",
        "#\n",
        "# Plug-in requirement:\n",
        "#   This cell assumes you have already run your previous cells and have:\n",
        "#     - sys1 : System1_SNC\n",
        "#     - codebook : CentralCodebookC2S\n",
        "#     - sem_enc : System1SemanticEncoder\n",
        "#   If not, it will create a tiny toy setup automatically.\n",
        "# =========================================================\n",
        "\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "import heapq\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0) Listener side semantic decoder: implement Eq (3) C2A\n",
        "# ---------------------------------------------------------\n",
        "def c2a_posterior_from_sys1(sys1, c: int, t: int, pA: Optional[np.ndarray] = None, eps: float = 1e-12) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    p(A=a | C=c; t) ∝ p(C=c | A=a; t) pA(a)\n",
        "    where p(C=c|A=a;t) is derived from Eq (2): normalize p_true[t,a,:] over concepts.\n",
        "    \"\"\"\n",
        "    A = sys1.A\n",
        "    C = sys1.C\n",
        "\n",
        "    if pA is None:\n",
        "        pA = np.ones(A, dtype=np.float64) / A\n",
        "    else:\n",
        "        pA = np.asarray(pA, dtype=np.float64)\n",
        "        pA = pA / np.sum(pA)\n",
        "\n",
        "    # p(C=c | A=a;t) for all actions a\n",
        "    w = sys1.p_true[t].astype(np.float64)              # (A, C)\n",
        "    denom = np.sum(w, axis=1, keepdims=True)           # (A,1)\n",
        "    pC_given_A = (w / denom)[:, c]                     # (A,)\n",
        "\n",
        "    unnorm = pC_given_A * pA\n",
        "    Z = np.sum(unnorm)\n",
        "    if Z <= eps:\n",
        "        return np.ones(A, dtype=np.float64) / A\n",
        "    return unnorm / Z\n",
        "\n",
        "def infer_action_map(sys1, concept_list: List[int], t: int, pA: Optional[np.ndarray] = None) -> int:\n",
        "    \"\"\"\n",
        "    If SR contains multiple concepts, combine evidence by multiplying posteriors\n",
        "    (log-add in practice). Here we do a simple product in log-domain.\n",
        "    \"\"\"\n",
        "    A = sys1.A\n",
        "    logp = np.zeros(A, dtype=np.float64)\n",
        "\n",
        "    # start from prior\n",
        "    if pA is None:\n",
        "        logp += -np.log(A)\n",
        "    else:\n",
        "        pA = np.asarray(pA, dtype=np.float64)\n",
        "        pA = pA / np.sum(pA)\n",
        "        logp += np.log(np.clip(pA, 1e-300, 1.0))\n",
        "\n",
        "    for c in concept_list:\n",
        "        post = c2a_posterior_from_sys1(sys1, c=c, t=t, pA=None)  # likelihood-like term\n",
        "        logp += np.log(np.clip(post, 1e-300, 1.0))\n",
        "\n",
        "    return int(np.argmax(logp))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1) Huffman source coding (lossless)\n",
        "# ---------------------------------------------------------\n",
        "@dataclass\n",
        "class HuffmanCode:\n",
        "    code: Dict[Any, str]   # symbol -> bitstring\n",
        "    trie: Dict             # decode trie\n",
        "\n",
        "def _make_trie(code: Dict[Any, str]) -> Dict:\n",
        "    root: Dict = {}\n",
        "    for sym, bits in code.items():\n",
        "        cur = root\n",
        "        for b in bits:\n",
        "            cur = cur.setdefault(b, {})\n",
        "        cur[\"$\"] = sym\n",
        "    return root\n",
        "\n",
        "def build_huffman_code(symbols: List[Any], probs: np.ndarray) -> HuffmanCode:\n",
        "    \"\"\"\n",
        "    Build a prefix-free Huffman code for the given symbol alphabet and probs.\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs, dtype=np.float64)\n",
        "    probs = probs / np.sum(probs)\n",
        "\n",
        "    # Priority queue items: (prob, unique_id, tree)\n",
        "    # tree is either a symbol or (left,right)\n",
        "    uid = 0\n",
        "    pq = []\n",
        "    for s, p in zip(symbols, probs):\n",
        "        heapq.heappush(pq, (float(p), uid, s))\n",
        "        uid += 1\n",
        "\n",
        "    # Handle degenerate alphabet\n",
        "    if len(pq) == 1:\n",
        "        s = pq[0][2]\n",
        "        code = {s: \"0\"}\n",
        "        return HuffmanCode(code=code, trie=_make_trie(code))\n",
        "\n",
        "    while len(pq) > 1:\n",
        "        p1, _, t1 = heapq.heappop(pq)\n",
        "        p2, _, t2 = heapq.heappop(pq)\n",
        "        heapq.heappush(pq, (p1 + p2, uid, (t1, t2)))\n",
        "        uid += 1\n",
        "\n",
        "    _, _, tree = pq[0]\n",
        "\n",
        "    # Traverse tree to assign codes\n",
        "    code: Dict[Any, str] = {}\n",
        "    def dfs(node, prefix):\n",
        "        if not isinstance(node, tuple):\n",
        "            code[node] = prefix if prefix != \"\" else \"0\"\n",
        "            return\n",
        "        left, right = node\n",
        "        dfs(left, prefix + \"0\")\n",
        "        dfs(right, prefix + \"1\")\n",
        "    dfs(tree, \"\")\n",
        "\n",
        "    return HuffmanCode(code=code, trie=_make_trie(code))\n",
        "\n",
        "def huffman_encode(seq: List[Any], hc: HuffmanCode) -> str:\n",
        "    return \"\".join(hc.code[s] for s in seq)\n",
        "\n",
        "def huffman_decode(bitstring: str, hc: HuffmanCode) -> List[Any]:\n",
        "    out = []\n",
        "    cur = hc.trie\n",
        "    for b in bitstring:\n",
        "        if b not in cur:\n",
        "            # decoding failure (due to bit errors)\n",
        "            raise ValueError(\"Huffman decode failed (prefix mismatch).\")\n",
        "        cur = cur[b]\n",
        "        if \"$\" in cur:\n",
        "            out.append(cur[\"$\"])\n",
        "            cur = hc.trie\n",
        "    # If we end not at root, leftover bits => failure\n",
        "    if cur is not hc.trie and cur != hc.trie:\n",
        "        # not necessarily fatal in all schemes, but treat as error here\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) Simple channel coding: repetition code (n=rep)\n",
        "# ---------------------------------------------------------\n",
        "def repetition_encode(bits: str, rep: int = 3) -> str:\n",
        "    assert rep % 2 == 1, \"Use odd repetition for majority vote.\"\n",
        "    return \"\".join(b * rep for b in bits)\n",
        "\n",
        "def repetition_decode(rx_bits: str, rep: int = 3) -> str:\n",
        "    assert len(rx_bits) % rep == 0\n",
        "    out = []\n",
        "    for i in range(0, len(rx_bits), rep):\n",
        "        block = rx_bits[i:i+rep]\n",
        "        ones = block.count(\"1\")\n",
        "        out.append(\"1\" if ones > rep//2 else \"0\")\n",
        "    return \"\".join(out)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3) Noisy channel: BSC (bit-flip with prob p)\n",
        "# ---------------------------------------------------------\n",
        "def bsc_channel(bits: str, flip_p: float, rng: np.random.Generator) -> str:\n",
        "    bits_arr = np.fromiter((1 if b == \"1\" else 0 for b in bits), dtype=np.int8)\n",
        "    flips = rng.random(bits_arr.shape[0]) < flip_p\n",
        "    bits_arr = bits_arr ^ flips.astype(np.int8)\n",
        "    return \"\".join(\"1\" if x else \"0\" for x in bits_arr)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4) Glue: end-to-end one-shot simulation\n",
        "# ---------------------------------------------------------\n",
        "@dataclass\n",
        "class E2EResult:\n",
        "    a_true: int\n",
        "    t: int\n",
        "    sr_symbols: List[Any]\n",
        "    sr_concepts: List[int]\n",
        "    src_bits: str\n",
        "    ch_bits: str\n",
        "    rx_ch_bits: str\n",
        "    dec_src_bits: str\n",
        "    sr_hat_symbols: List[Any]\n",
        "    sr_hat_concepts: List[int]\n",
        "    a_hat: Optional[int]\n",
        "    success: bool\n",
        "    notes: str\n",
        "\n",
        "def simulate_system1_e2e(\n",
        "    sys1,\n",
        "    sem_enc,\n",
        "    codebook,\n",
        "    t: int,\n",
        "    a_true: int,\n",
        "    k_concepts: int = 1,\n",
        "    semantic_mode: str = \"sample\",\n",
        "    pA: Optional[np.ndarray] = None,\n",
        "    flip_p: float = 0.02,\n",
        "    rep: int = 3,\n",
        "    rng: Optional[np.random.Generator] = None\n",
        ") -> E2EResult:\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "      action -> SR(symbols)\n",
        "      source coding (Huffman)\n",
        "      channel coding (repetition)\n",
        "      noisy channel (BSC)\n",
        "      channel decode\n",
        "      source decode\n",
        "      SR_hat -> action_hat\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng(0)\n",
        "\n",
        "    # ---- semantic encoder (System 1) ----\n",
        "    sr = sem_enc.encode(a=a_true, t=t, k=k_concepts, mode=semantic_mode)\n",
        "\n",
        "    # ---- build Huffman code from q distribution over symbols ----\n",
        "    # Use A2C mixture distribution as a proxy for symbol frequency:\n",
        "    #   q_c ∝ p_Xc(TRUE) = Σ_a p_true[t,a,c] pA(a)\n",
        "    # Here we compute q over concepts then transfer to symbols via codebook.\n",
        "    A = sys1.A\n",
        "    C = sys1.C\n",
        "    if pA is None:\n",
        "        pA_use = np.ones(A, dtype=np.float64) / A\n",
        "    else:\n",
        "        pA_use = np.asarray(pA, dtype=np.float64)\n",
        "        pA_use = pA_use / np.sum(pA_use)\n",
        "\n",
        "    p_x_true = (sys1.p_true[t] * pA_use[:, None]).sum(axis=0)  # (C,)\n",
        "    denom = np.sum(p_x_true)\n",
        "    if denom <= 0:\n",
        "        q = np.ones(C, dtype=np.float64) / C\n",
        "    else:\n",
        "        q = p_x_true / denom\n",
        "\n",
        "    # alphabet: all symbols in codebook (must be shared / centralized)\n",
        "    symbol_alphabet = [codebook.c2s[c] for c in range(C)]\n",
        "    hc = build_huffman_code(symbol_alphabet, q)\n",
        "\n",
        "    # ---- source encode SR symbols -> bits ----\n",
        "    try:\n",
        "        src_bits = huffman_encode(sr.symbols, hc)\n",
        "    except KeyError as e:\n",
        "        return E2EResult(\n",
        "            a_true=a_true, t=t,\n",
        "            sr_symbols=sr.symbols, sr_concepts=sr.concepts,\n",
        "            src_bits=\"\", ch_bits=\"\", rx_ch_bits=\"\", dec_src_bits=\"\",\n",
        "            sr_hat_symbols=[], sr_hat_concepts=[],\n",
        "            a_hat=None, success=False,\n",
        "            notes=f\"Source encode failed: unknown symbol {e}\"\n",
        "        )\n",
        "\n",
        "    # ---- channel encode ----\n",
        "    ch_bits = repetition_encode(src_bits, rep=rep)\n",
        "\n",
        "    # ---- noisy channel ----\n",
        "    rx_ch_bits = bsc_channel(ch_bits, flip_p=flip_p, rng=rng)\n",
        "\n",
        "    # ---- channel decode ----\n",
        "    dec_src_bits = repetition_decode(rx_ch_bits, rep=rep)\n",
        "\n",
        "    # ---- source decode -> symbols ----\n",
        "    try:\n",
        "        sr_hat_symbols = huffman_decode(dec_src_bits, hc)\n",
        "    except Exception as e:\n",
        "        return E2EResult(\n",
        "            a_true=a_true, t=t,\n",
        "            sr_symbols=sr.symbols, sr_concepts=sr.concepts,\n",
        "            src_bits=src_bits, ch_bits=ch_bits, rx_ch_bits=rx_ch_bits, dec_src_bits=dec_src_bits,\n",
        "            sr_hat_symbols=[], sr_hat_concepts=[],\n",
        "            a_hat=None, success=False,\n",
        "            notes=f\"Huffman decode failed (likely due to residual bit errors): {e}\"\n",
        "        )\n",
        "\n",
        "    # ---- S2C (deterministic inverse) ----\n",
        "    try:\n",
        "        sr_hat_concepts = codebook.decode_symbols_to_concepts(sr_hat_symbols).tolist()\n",
        "    except Exception as e:\n",
        "        return E2EResult(\n",
        "            a_true=a_true, t=t,\n",
        "            sr_symbols=sr.symbols, sr_concepts=sr.concepts,\n",
        "            src_bits=src_bits, ch_bits=ch_bits, rx_ch_bits=rx_ch_bits, dec_src_bits=dec_src_bits,\n",
        "            sr_hat_symbols=sr_hat_symbols, sr_hat_concepts=[],\n",
        "            a_hat=None, success=False,\n",
        "            notes=f\"S2C failed: {e}\"\n",
        "        )\n",
        "\n",
        "    # ---- semantic decode: concepts -> action_hat (MAP) ----\n",
        "    a_hat = infer_action_map(sys1, concept_list=sr_hat_concepts, t=t, pA=pA_use)\n",
        "\n",
        "    success = (a_hat == a_true)\n",
        "\n",
        "    return E2EResult(\n",
        "        a_true=a_true, t=t,\n",
        "        sr_symbols=sr.symbols, sr_concepts=sr.concepts,\n",
        "        src_bits=src_bits, ch_bits=ch_bits, rx_ch_bits=rx_ch_bits, dec_src_bits=dec_src_bits,\n",
        "        sr_hat_symbols=sr_hat_symbols, sr_hat_concepts=sr_hat_concepts,\n",
        "        a_hat=a_hat, success=success,\n",
        "        notes=\"OK\"\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# If your previous objects aren't defined, create a toy setup.\n",
        "# =========================================================\n",
        "try:\n",
        "    sys1\n",
        "    codebook\n",
        "    sem_enc\n",
        "except NameError:\n",
        "    print(\"Previous objects not found; creating toy sys1/codebook/sem_enc.\")\n",
        "    # Minimal toy setup using your class definitions (must be in scope)\n",
        "    T, A, C = 2, 6, 12\n",
        "    sys1 = System1_SNC(num_tasks=T, num_actions=A, num_concepts=C, seed=123)\n",
        "    sys1.random_init_beta(alpha=0.6, beta=0.6)\n",
        "    codebook = CentralCodebookC2S.make_tokenized(num_concepts=C, prefix=\"SYM\")\n",
        "    sem_enc = System1SemanticEncoder(sys1=sys1, codebook=codebook)\n",
        "\n",
        "# =========================================================\n",
        "# Run a small simulation sweep\n",
        "# =========================================================\n",
        "rng = np.random.default_rng(2026)\n",
        "\n",
        "t = 0\n",
        "pA = np.ones(sys1.A, dtype=np.float64) / sys1.A\n",
        "\n",
        "flip_p = 0.02     # channel noise\n",
        "rep = 3           # repetition code strength (odd)\n",
        "k_concepts = 1    # paper default is 1; try 3 to see robustness change\n",
        "\n",
        "num_trials = 30\n",
        "hits = 0\n",
        "decode_fail = 0\n",
        "\n",
        "print(\"=== System 1 SNC end-to-end with channel (simple) ===\")\n",
        "print(f\"flip_p={flip_p}, repetition={rep}, k_concepts={k_concepts}\\n\")\n",
        "\n",
        "for i in range(num_trials):\n",
        "    a_true = int(rng.integers(low=0, high=sys1.A))\n",
        "    r = simulate_system1_e2e(\n",
        "        sys1=sys1, sem_enc=sem_enc, codebook=codebook,\n",
        "        t=t, a_true=a_true,\n",
        "        k_concepts=k_concepts, semantic_mode=\"sample\",\n",
        "        pA=pA, flip_p=flip_p, rep=rep, rng=rng\n",
        "    )\n",
        "    if r.notes.startswith(\"Huffman decode failed\"):\n",
        "        decode_fail += 1\n",
        "    if r.success:\n",
        "        hits += 1\n",
        "\n",
        "    if i < 3:\n",
        "        print(f\"[Trial {i}] a_true={r.a_true} -> a_hat={r.a_hat}, success={r.success}\")\n",
        "        print(f\"  SR symbols: {r.sr_symbols}\")\n",
        "        print(f\"  src_bits(len={len(r.src_bits)}): {r.src_bits[:80]}{'...' if len(r.src_bits)>80 else ''}\")\n",
        "        print(f\"  ch_bits(len={len(r.ch_bits)}): {r.ch_bits[:80]}{'...' if len(r.ch_bits)>80 else ''}\")\n",
        "        print(f\"  rx_bits(len={len(r.rx_ch_bits)}): {r.rx_ch_bits[:80]}{'...' if len(r.rx_ch_bits)>80 else ''}\")\n",
        "        print(f\"  SR_hat symbols: {r.sr_hat_symbols}\")\n",
        "        print(f\"  note: {r.notes}\\n\")\n",
        "\n",
        "print(f\"Accuracy (MAP action) = {hits}/{num_trials} = {hits/num_trials:.3f}\")\n",
        "print(f\"Source decode failures = {decode_fail}/{num_trials} = {decode_fail/num_trials:.3f}\")\n",
        "\n",
        "print(\"\\nTip: increase repetition (e.g., rep=5) or reduce flip_p to improve reliability, \"\n",
        "      \"at the cost of longer bitstream (compromising minimality), matching the paper's narrative.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC0C0YVPKPA5",
        "outputId": "7ddd92f8-dc2e-40d1-99a4-5fb69f78b95c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== System 1 SNC end-to-end with channel (simple) ===\n",
            "flip_p=0.02, repetition=3, k_concepts=1\n",
            "\n",
            "[Trial 0] a_true=4 -> a_hat=0, success=False\n",
            "  SR symbols: ['SYM_0005']\n",
            "  src_bits(len=4): 1100\n",
            "  ch_bits(len=12): 111111000000\n",
            "  rx_bits(len=12): 111111000000\n",
            "  SR_hat symbols: ['SYM_0005']\n",
            "  note: OK\n",
            "\n",
            "[Trial 1] a_true=0 -> a_hat=4, success=False\n",
            "  SR symbols: ['SYM_0008']\n",
            "  src_bits(len=4): 1101\n",
            "  ch_bits(len=12): 111111000111\n",
            "  rx_bits(len=12): 111111000101\n",
            "  SR_hat symbols: ['SYM_0008']\n",
            "  note: OK\n",
            "\n",
            "[Trial 2] a_true=4 -> a_hat=4, success=True\n",
            "  SR symbols: ['SYM_0009']\n",
            "  src_bits(len=3): 010\n",
            "  ch_bits(len=9): 000111000\n",
            "  rx_bits(len=9): 000111000\n",
            "  SR_hat symbols: ['SYM_0009']\n",
            "  note: OK\n",
            "\n",
            "Accuracy (MAP action) = 14/30 = 0.467\n",
            "Source decode failures = 0/30 = 0.000\n",
            "\n",
            "Tip: increase repetition (e.g., rep=5) or reduce flip_p to improve reliability, at the cost of longer bitstream (compromising minimality), matching the paper's narrative.\n"
          ]
        }
      ]
    }
  ]
}